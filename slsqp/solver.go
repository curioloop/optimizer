// Copyright Â©2025 curioloop. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package slsqp

import (
	"math"
)

// sqpSolver solve NLP(general constrained NonLinear optimization Problem) with SQP(Sequential Quadratic Programming)
//
// minimize ğ’‡(ğ±) subject to
//   - equality constrains: ğ’„â±¼(ğ±) = 0  (j = 1 Â·Â·Â· mâ‚‘)
//   - inequality constrains: ğ’„â±¼(ğ±) â‰¥ 0  (j = mâ‚‘+1 Â·Â·Â· m)
//   - boundaries: ğ’áµ¢ â‰¤ ğ±áµ¢ â‰¤ ğ’–áµ¢ (i = 1 Â·Â·Â· n)
//
// SQP decomposes NLP into a series of QP sub-problems,
// each of which solves a descent direction ğ and step length ğ›‚,
// and ensures that ğ’‡(ğ± + ğ›‚ğ) < ğ’‡(ğ±) and the updated ğ± satisfies the constraints.
//
// # Direction
//
// The Lagrangian function of NLS is given by â„’(ğ±,ğ›Œ) = ğ’‡(ğ±) - âˆ‘ğ›Œâ±¼ğ’„â±¼(ğ±)
// which is a linear approximation of constraints ğ’„â±¼(ğ±).
//
// A quadratic approximation of â„’(ğ±,ğ›Œ) at location ğ±áµ is a standard form of QP problem:
//
// minimize Â½ ğáµ€ğáµğ + ğœµğ’‡(ğ±áµ)ğ subject to
//   - ğœµğ’„â±¼(ğ±áµ)ğ + ğ’„â±¼(ğ±áµ) = 0  (j = 1 Â·Â·Â· mâ‚‘)
//   - ğœµğ’„â±¼(ğ±áµ)ğ + ğ’„â±¼(ğ±áµ) â‰¥ 0  (j = mâ‚‘+1 Â·Â·Â· m)
//
// With a symmetric Hessian approximation ğáµ â‰ˆ ğœµÂ²â„’(ğ±áµ,ğ›Œáµ),
// the descent search direction ğ is determined by above problem.
//
// # Inconsistent Constraints
//
// The constraints in QP might become inconsistent with original NLP during the iteration.
// To overcome this difficulty, an augmented QP relaxation with slack variable ğ›… is introduced to ensure consistency.
//
// minimize Â½ ğáµ€ğáµğ + ğœµğ’‡(ğ±áµ)ğ + Â½ğ›’ğ›…Â² subject to
//   - ğœµğ’„â±¼(ğ±áµ)ğ + ğ’„â±¼(ğ±áµ) + ğ›…ğ’„â±¼(ğ±áµ) = 0  (j = 1 Â·Â·Â· mâ‚‘)
//   - ğœµğ’„â±¼(ğ±áµ)ğ + ğ’„â±¼(ğ±áµ) + ğ›…ğ›‡â±¼ğ’„â±¼(ğ±áµ) â‰¥ 0  (j = mâ‚‘+1 Â·Â·Â· m)
//   - 0 â‰¤ ğ›… â‰¤ 1
//   - ğ›‡â±¼ = 0 if ğ’„â±¼(ğ±áµ) > 0 (j = mâ‚‘+1 Â·Â·Â· m)
//   - ğ›‡â±¼ = 1 if ğ’„â±¼(ğ±áµ) â‰¤ 0 (j = mâ‚‘+1 Â·Â·Â· m)
//
// where
//   - 10Â² â‰¤ ğ›’ â‰¤ 10â· is a constant to penalize the violation of the linear constraints
//   - ğš­ is an (m-mâ‚‘)Ã—(m-mâ‚‘) diagonal selection matrix with diagonal elements ğš­â±¼â±¼ = ğ›‡â±¼
//
// The augmented direction is given by (n+1)-vector [ğ ğ›…]áµ€ with initial value [0 Â·Â·Â· 0 1]áµ€.
// Note that augmented QP is always feasible because ğ = 0 and ğ›… = 1 can satisfy its constraints trivially.
//
// # Step
//
// The step length ğ›‚ is obtained by minimize merit function ğ¿(ğ›‚) = ğŸ‡(ğ± + ğ›‚ğ)
// where ğŸ‡(ğ±;ğ›’) is a non-differentiable function with L1 penalty ğŸ‡(ğ±;ğ›’) = ğ’‡(ğ±) + âˆ‘ğ›’â±¼â€–ğ’„â±¼(ğ±)â€–â‚
//   - â€– ğ’„â±¼(ğ±) â€–â‚ = ğšŠğš‹ğšœ[ğ’„â±¼(ğ±)] = ğš–ğšŠğš¡[ğ’„â±¼(ğ±),-ğ’„â±¼(ğ±)]    (j = 1 Â·Â·Â· mâ‚‘)
//   - â€– ğ’„â±¼(ğ±) â€–â‚ = ğšŠğš‹ğšœ[ğš–ğš’ğš—[0,ğ’„â±¼(ğ±)]] = ğš–ğšŠğš¡[0,-ğ’„â±¼(ğ±)] (j = mâ‚‘+1 Â·Â·Â· m)
//
// Maximize the penalty parameters ğ›’ iteratively could lead to optimal solution
//
//	ğ›’â±¼áµâºÂ¹ = ğš–ğšŠğš¡[ Â½(ğ›’â±¼áµ+|ğ›Œâ±¼|), |ğ›Œâ±¼| ] (j = 1 Â·Â·Â· m)
//
// where ğ›â±¼ is the Lagrange multiplier of j-th constraint.
//
// To overcome possible difficulties in the line search of non-differentiable merit function,
// the update iteration of ğ›’â±¼ is substituted by the differentiable augmented Lagrangian function
//
//	                â§ -âˆ‘(ğ›Œâ±¼ğ’„â±¼(ğ±) - Â½ğ›’â±¼ğ’„â±¼Â²(ğ±))  âˆ€j = 1 Â·Â·Â· mâ‚‘
//	ğ¥(ğ±;ğ›’) = ğ’‡(ğ±) + â¨ -âˆ‘(ğ›Œâ±¼ğ’„â±¼(ğ±) - Â½ğ›’â±¼ğ’„â±¼Â²(ğ±))  âˆ€j = 1+mâ‚‘ Â·Â·Â· m and ğ’„â±¼(ğ±) â‰¤ ğ›Œâ±¼/ğ›’â±¼
//	                â© -âˆ‘(Â½ğ›Œâ±¼Â²/ğ›’â±¼Â²)            âˆ€j = 1+mâ‚‘ Â·Â·Â· m and ğ’„â±¼(ğ±) > ğ›Œâ±¼/ğ›’â±¼
//
// then the directional derivative of the merit function along the ğ is given by:
//
//	ğœµğ¥(ğ;ğ±áµ,ğ›’áµ) = ğœµğ’‡(ğ±áµ)áµ€ğ - âˆ‘ğ›’áµâ±¼â€–ğ’„â±¼(ğ±áµ)â€–â‚
//
// Finally the step length ğ›‚ is obtained by performing line-search along ğœµğ¥(ğ±,ğ›Œ;ğ›’) with Armijio condition:
//
//	ğ¥(ğ±áµ+ğ›‚ğ;ğ›Œ,ğ›’) - ğ¥(ğ±áµ;ğ›Œ,ğ›’) < Î· Â· ğ›‚ Â· ğœµğ¥(ğ;ğ±áµ,ğ›’áµ) (0<Î·<0.5)
//
// # Least Squares Sub-Problem
//
// The quasi-newton method BFGS is suitable for it only uses first-order information to approximate the hesse-matrix ğ of Lagrangian function.
// In constrained optimization, ğ > 0 is required to ensure convex. Hence a modified BFGS formula is used:
//   - ğáµâºÂ¹ = ğáµ + ğªğªáµ€/ğªáµ€ğ¬ + ğáµğ¬ğ¬áµ€ğáµ/ğ¬áµ€ğáµğ¬
//   - ğ¬ = ğ±áµâºÂ¹ - ğ±áµ
//   - ğª = ğ›‰ğ›ˆ + (1-ğ›‰)ğáµğ¬
//   - ğ›ˆ = ğœµâ„’(ğ±áµâºÂ¹,ğ›Œáµ) - ğœµâ„’(ğ±áµ,ğ›Œáµ)
//   - if ğ¬áµ€ğ›ˆ â‰¥ â…• ğ¬áµ€ğáµğ¬ : ğ›‰ = 1
//   - otherwise : ğ›‰ = â…˜ ğ¬áµ€ğáµğ¬ / (ğ¬áµ€ğáµğ¬ - ğ¬áµ€ğ›ˆ)
//
// In practice, the matrix is presented as ğ = ğ‹ğƒğ‹áµ€
//   - ğ‹ is a strict lower triangular
//   - ğƒ is diagonal matrix
//
// By using ğ‹ğƒğ‹áµ€ factorization, the QP sub-problem could be replace by a linear least squares sub-problem:
//
// minimize â€– ğƒÂ¹áŸÂ²ğ‹áµ€ğ + ğƒâ»Â¹áŸÂ²ğ‹â»Â¹ğœµğ’‡(ğ±áµ) â€–â‚‚
//
// subject to
//   - ğœµğ’„â±¼(ğ±áµ)ğ + ğ’„â±¼(ğ±áµ) = 0  (j = 1 Â·Â·Â· mâ‚‘)
//   - ğœµğ’„â±¼(ğ±áµ)ğ + ğ’„â±¼(ğ±áµ) â‰¥ 0  (j = mâ‚‘+1 Â·Â·Â· m)
//
// The augment QP sub-problem with
//
//	minimize â€– â¡ ğƒÂ¹áŸÂ²ğ‹áµ€  O â¤â¡ ğ â¤ + â¡ ğƒâ»Â¹áŸÂ²ğ‹â»Â¹ğœµğ’‡(ğ±áµ)â¤ â€–
//	         â€– â£ O    ğ›’Â¹áŸÂ² â¦â£ ğ›… â¦ + â£       O       â¦ â€–â‚‚
//
// subject to
//   - ğœµğ’„(ğ±áµ)ğ + ğ’„(ğ±áµ) - ğ›…ğ’„(ğ±áµ) = 0  (j = 1 Â·Â·Â· mâ‚‘)
//   - ğœµğ’„(ğ±áµ)ğ + ğ’„(ğ±áµ) - ğ›…ğš­ğ’„(ğ±áµ) â‰¥ 0  (j = mâ‚‘+1 Â·Â·Â· m)
//
// # Convergence Criteria
//
// The KKT conditions cannot be satisfied within the required tolerance for many real-world problems due to its scale variance.
//
// SLSQP code in Scipy check convergence by the following three aspects:
//   - feasibility : the summation of violation in all the constraints.
//   - optimality : the decrease potential of the objective function and the weighted constraint infeasibility.
//   - step-length : the 2-norm of the descent direction.
//
// Below criteria are checked after obtaining the solution ğ to the problem QP:
//   - Cğ‘£ğ‘–ğ‘œ = âˆ‘â€–ğ’„â±¼(ğ±áµ)â€–â‚
//   - Cğ‘œğ‘ğ‘¡ = |ğœµğ’‡(ğ±áµ)áµ€ğ| + |ğ›Œáµ|áµ€Ã—â€–ğ’„(ğ±áµ)â€–â‚
//   - Cğ‘ ğ‘¡ğ‘ = â€–ğâ€–â‚‚
//
// Below criteria are checked after line-search found the step ğ›‚:
//   - Äˆğ‘£ğ‘–ğ‘œ = âˆ‘â€–ğ’„â±¼(ğ±áµ + ğ›‚ğ)â€–â‚
//   - Äˆğ‘œğ‘ğ‘¡ = |ğ’‡(ğ±áµ + ğ›‚ğ) - ğ’‡(ğ±áµ)|
//   - Äˆğ‘ ğ‘¡ğ‘ = â€–ğâ€–â‚‚
//
// # Reference
//
// Dieter Kraft: "A software package for sequential quadratic programming".
// DFVLR-FB 88-28, 1988
type sqpSolver struct {
	optimizer *Optimizer
	workspace *Workspace
	location  *sqpLoc
}

func (ss *sqpSolver) evalLoc(mode sqpMode) sqpMode {
	o, loc := ss.optimizer, ss.location
	func() {
		defer func() {
			if r := recover(); r != nil {
				mode = BadArgument
			}
		}()
		switch mode {
		case evalFunc:
			loc.f = o.Object(loc.x, nil)
			for j, cons := range o.EqCons {
				loc.c[j] = cons(loc.x, nil)
			}
			for j, cons := range o.NeqCons {
				loc.c[j+o.meq] = cons(loc.x, nil)
			}
		case evalGrad:
			tmp, mda := loc.g[:o.n], max(o.m, 1)
			for i, cons := range o.EqCons {
				cons(loc.x, tmp)
				dcopy(o.n, tmp, 1, loc.a[i:], mda)
			}
			for i, cons := range o.NeqCons {
				cons(loc.x, tmp)
				dcopy(o.n, tmp, 1, loc.a[i+o.meq:], mda)
			}
			o.Object(loc.x, loc.g[:o.n])
		default:
			mode = BadArgument
			return
		}
		mode = OK
	}()
	return mode
}

func (ss *sqpSolver) initCtx() (mode sqpMode) {

	if mode = ss.evalLoc(evalFunc); mode != OK {
		return
	}
	if mode = ss.evalLoc(evalGrad); mode != OK {
		return
	}

	// Initialization for the first iteration
	s, c := &ss.optimizer.sqpSpec, &ss.workspace.sqpCtx
	c.acc = s.Stop.Accuracy
	c.tol = ten * c.acc
	c.iter = 0
	c.reset = 0
	dzero(c.s)
	dzero(c.mu)
	return ss.resetBFGS()
}

func (ss *sqpSolver) resetBFGS() (mode sqpMode) {
	spec, ctx := &ss.optimizer.sqpSpec, &ss.workspace.sqpCtx
	ctx.reset++
	if ctx.reset > 5 {
		// Check relaxed convergence in case of positive directional derivative.
		_, mode = ss.checkConv(ctx.tol, SearchNotDescent)
	} else {
		// ğ‹ = ğˆ , ğƒ = ğˆ
		l, n := ctx.l, spec.n
		n2 := (n + 1) * n / 2
		dzero(l[:n2])
		for i, j := 0, 0; i < n; i++ {
			l[j] = one
			j += n - i // diag
		}
	}
	return mode
}

func (ss *sqpSolver) checkConv(tol float64, notConv sqpMode) (h3 float64, mode sqpMode) {
	meq := ss.optimizer.sqpSpec.meq
	for j, c := range ss.location.c {
		h1 := zero
		if j < meq {
			h1 = c
		}
		h3 += math.Max(-c, h1)
	}
	if !ss.checkStop(h3, tol) {
		mode = notConv
	}
	return
}

func (ss *sqpSolver) checkStop(vio, tol float64) bool {
	spec, ctx, loc := &ss.optimizer.sqpSpec, &ss.workspace.sqpCtx, ss.location
	// Äˆğ‘£ğ‘–ğ‘œ = âˆ‘â€–ğ’„â±¼(ğ±áµ + ğ›‚ğ)â€–â‚
	if vio >= tol || ctx.bad || math.IsNaN(loc.f) {
		return false
	} else {
		stop := spec.Stop
		switch {
		case math.Abs(loc.f-ctx.f0) < tol: // Äˆğ‘œğ‘ğ‘¡ = |ğ’‡(ğ±áµ + ğ›‚ğ) - ğ’‡(ğ±áµ)|
			return true
		case dnrm2(spec.n, ctx.s, 1) < tol: // Äˆğ‘ ğ‘¡ğ‘ = â€–ğâ€–â‚‚
			return true
		case stop.FEvalTolerance >= zero && math.Abs(loc.f) < stop.FEvalTolerance:
			return true
		case stop.FDiffTolerance >= zero && math.Abs(loc.f-ctx.f0) < stop.FDiffTolerance:
			return true
		case stop.XDiffTolerance >= zero:
			n, x, x0, u := spec.n, loc.x, ctx.x0, ctx.u
			dcopy(n, x, 1, u, 1)
			daxpy(n, -1, x0, 1, u, 1)
			return dnrm2(n, u, 1) < stop.XDiffTolerance
		}
		return false
	}
}

func (ss *sqpSolver) updateBFGS() (mode sqpMode) {

	// set loc.g = ğœµğ’‡(ğ±) and loc.a = ğœµğ’„(ğ±)
	if mode = ss.evalLoc(evalGrad); mode != OK {
		return
	}

	// Update Cholesky-factors of the Hessian matrix by modified BFGS formula
	spec, ctx, loc := &ss.optimizer.sqpSpec, &ss.workspace.sqpCtx, ss.location

	m, n, la := spec.m, spec.n, max(spec.m, 1)
	u, r, v, l, s := ctx.u, ctx.r, ctx.v, ctx.l, ctx.s

	if n < 0 || n > len(v) || n > len(u) {
		panic("bound check error")
	}

	// ğ›ˆ = ğœµâ„’(ğ±áµâºÂ¹,ğ›Œáµ) - ğœµâ„’(ğ±áµ,ğ›Œáµ)
	//   = [ğœµğ’‡(ğ±áµâºÂ¹) - ğ›Œğœµğ’„(ğ±áµâºÂ¹)] - [ğœµğ’‡(ğ±áµ) - ğ›Œğœµğ’„(ğ±áµ)]
	for i, g := range loc.g[:n] {
		u[i] = g - ddot(m, loc.a[i*la:(i+1)*la], 1, r, 1) - v[i]
	}

	// ğ‹áµ€ğ¬
	for i, k := 0, 0; i < n; i++ {
		k++
		sm := zero
		for _, s := range s[i+1 : n] {
			sm += l[k] * s
			k++
		}
		v[i] = s[i] + sm
	}
	// ğƒğ‹áµ€ğ¬
	for i, k := 0, 0; i < n; i++ {
		v[i] = l[k] * v[i]
		k += n - i
	}
	// ğ‹ğƒğ‹áµ€ğ¬ = ğáµğ¬
	for i := n - 1; i >= 0; i-- {
		k := i
		sm := zero
		for j, v := range v[:i] {
			sm += l[k] * v
			k += n - 1 - j
		}
		v[i] += sm
	}

	h1 := ddot(n, s, 1, u, 1) // ğ¬áµ€ğ›ˆ
	h2 := ddot(n, s, 1, v, 1) // ğ¬áµ€ğáµğ¬
	h3 := 0.2 * h2
	if h1 < h3 {
		// ğ›‰ =  â…˜ ğ¬áµ€ğáµğ¬ / (ğ¬áµ€ğáµğ¬ - ğ¬áµ€ğ›ˆ)
		h4 := (h2 - h3) / (h2 - h1)
		h1 = h3
		dscal(n, h4, u, 1)           // ğ›‰ğ¬áµ€ğ›ˆ
		daxpy(n, one-h4, v, 1, u, 1) // ğ›‰ğ¬áµ€ğ›ˆ + ğ¬áµ€(1-ğ›‰)ğáµğ¬ = ğ¬áµ€(ğ›‰ğ›ˆ + (1-ğ›‰)ğáµğ¬) = ğ¬áµ€ğª
	}

	if h1 == zero || h2 == zero {
		mode = ss.resetBFGS()
		if ctx.reset > 5 {
			return
		}
	} else {
		// if ğ›‰ = 1 : Ïƒğ³ğ³áµ€ = ğ¬áµ€ğª(ğ¬áµ€ğª)áµ€ / â…•ğ¬áµ€ğáµğ¬
		// otherwise : Ïƒğ³ğ³áµ€ = ğ›ˆğ›ˆáµ€ / ğ¬áµ€ğ›ˆ
		compositeT(uint(n), l, u, +one/h1, nil)
		// Ïƒğ³ğ³áµ€ = ğáµğ¬(ğáµğ¬)áµ€ / ğ¬áµ€ğáµğ¬ = ğáµğ¬ğ¬áµ€ğáµ / ğ¬áµ€ğáµğ¬
		compositeT(uint(n), l, v, -one/h2, u)
	}

	return
}

// SLSQP (Sequential Least Squares Programming) to solve general nonlinear optimization problems.
func (ss *sqpSolver) mainLoop() (mode sqpMode) {

	loc := ss.location
	ctx := &ss.workspace.sqpCtx
	spec := &ss.optimizer.sqpSpec

	n1 := spec.n + 1
	n2 := spec.n * n1 / 2

	m, meq, n, la := spec.m, spec.meq, spec.n, max(spec.m, 1)
	u, r, v, l, s := ctx.u, ctx.r, ctx.v, ctx.l, ctx.s

	mode = ss.initCtx()
	for mode == OK {

		if ctx.iter++; ctx.iter > spec.Stop.MaxIterations {
			ctx.iter--
			return SQPExceedMaxIter
		}

		// Solve an mÃ—n QP sub-problem to obtained ğ and ğ›Œ
		// then set ctx.s = ğ and ctx.r = ğ›Œ

		// Transfer bounds from ğ’ â‰¤ ğ± â‰¤ ğ’– to ğ’ - ğ±áµ â‰¤ ğ â‰¤ ğ’– - ğ±áµ
		for i, b := range spec.Bounds {
			x := loc.x[i]
			u[i] = b.Lower - x // ğ±áµ + ğ â‰¥ ğ’  â†’  ğ â‰¥ ğ’ - ğ±áµ
			v[i] = b.Upper - x // ğ±áµ + ğ â‰¤ ğ’–  â†’  ğ â‰¤ ğ’– - ğ±áµ
		}
		_, mode = LSQ(m, meq, n, n2+1,
			l, loc.g, loc.a, loc.c, u, v,
			s, r, ctx.w, ctx.jw, spec.Stop.NNLSIterations, spec.BndInf)

		if mode == LSEISingularC && n == meq {
			mode = ConsIncompatible
		}
		h4 := one
		// If it turns out that the original SQP problem is inconsistent,
		// set ctx.bad = true to prevent termination with convergence on this iteration,
		// even if the augmented problem was solved.
		if ctx.bad = mode == ConsIncompatible; ctx.bad {
			// Form augmented QP relaxation.
			a := loc.a[n*la : n1*la]
			for j, c := range loc.c[:m] {
				if j < meq {
					a[j] = -c // -ğ’„â±¼(ğ±áµ)
				} else {
					a[j] = math.Max(-c, zero) // -ğ›‡â±¼ğ’„â±¼(ğ±áµ)
				}
			}
			loc.g[n] = zero
			l[n2] = hun            // ğ›’ = 10Â²
			dzero(s[:n])           // ğ = 0
			s[n] = one             // ğ›… = 1
			u[n], v[n] = zero, one // 0 â‰¤ ğ›… â‰¤ 1

			for relax := 0; relax <= 5; relax++ {
				// Solve mÃ—(n+1) augmented problem
				_, mode = LSQ(m, meq, n1, n2+1, l, loc.g, loc.a, loc.c, u, v,
					s, r, ctx.w, ctx.jw, spec.Stop.NNLSIterations, spec.BndInf)
				h4 = one - s[n] // 1 - ğ›…
				if mode == ConsIncompatible {
					l[n2] *= ten // ğ›’ = ğ›’ Ã— 10
					continue
				}
				break
			}
		}

		// Unable to solve LSQ even the augmented one.
		if mode != HasSolution {
			return
		}

		// Update multipliers for L1-test
		for i, g := range loc.g[:n] {
			// save ctx.r = ğœµğ’‡(ğ±áµ) - ğ›Œğœµğ’„(ğ±áµ) for BFGS update
			v[i] = g - ddot(m, loc.a[i*la:(i+1)*la], 1, r, 1)
		}

		ctx.f0 = loc.f
		copy(ctx.x0, loc.x)

		gs := ddot(n, loc.g, 1, s, 1) // ğœµğ’‡(ğ±áµ)áµ€ğ
		h1 := math.Abs(gs)            // Cğ‘œğ‘ğ‘¡ = |ğœµğ’‡(ğ±áµ)áµ€ğ| + |ğ›Œáµ|áµ€Ã—â€–ğ’„(ğ±áµ)â€–â‚
		h2 := zero                    // Cğ‘£ğ‘–ğ‘œ = âˆ‘â€–ğ’„â±¼(ğ±áµ)â€–â‚
		for j, c := range loc.c[:m] {
			h3 := zero
			if j < meq {
				h3 = c
			}
			h2 += math.Max(-c, h3)                     // â€–ğ’„â±¼(ğ±áµ)â€–â‚
			h3 = math.Abs(r[j])                        // |ğ›Œâ±¼|
			h1 += h3 * math.Abs(c)                     // |ğ›Œâ±¼|Ã—â€–ğ’„â±¼(ğ±áµ)â€–â‚
			ctx.mu[j] = math.Max(h3, (ctx.mu[j]+h3)/2) // ğ›’â±¼áµâºÂ¹ = ğš–ğšŠğš¡[ Â½(ğ›’â±¼áµ+|ğ›Œâ±¼|), |ğ›Œâ±¼| ]
		}

		// Check the convergence criteria for NLP problem,
		// stop if they are satisfied
		if h1 < ctx.acc && h2 < ctx.acc && !ctx.bad && !math.IsNaN(loc.f) {
			return OK
		}

		h1 = zero // âˆ‘ğ›’áµâ±¼â€–ğ’„â±¼(ğ±áµ)â€–â‚
		for j, c := range loc.c[:m] {
			h3 := zero
			if j < meq {
				h3 = c
			}
			h1 += ctx.mu[j] * math.Max(-c, h3) // â€–ğ’„â±¼(ğ±áµ)â€–â‚
		}

		// ğ¥(ğ±áµ;ğ›’) = ğ’‡(ğ±áµ) + ğ›’áµâ€–ğ’„(ğ±áµ)â€–â‚
		ctx.t0 = loc.f + h1

		// ğœµğ¥ = ğœµğ’‡(ğ±áµ)áµ€ğ - (1 - ğ›…)âˆ‘ğ›’áµâ±¼â€–ğ’„â±¼(ğ±áµ)â€–â‚
		h3 := gs - h1*h4
		if h3 >= zero {
			// Reset the Hessian matrix when an ascent direction is generated.
			mode = ss.resetBFGS()
			if ctx.reset > 5 {
				return
			}
			continue
		}

		// Conduct the line search with the merit function to get a step length ğ›‚,
		// set ğ±áµâºÂ¹ = ğ±áµ + ğ›‚ğ and evaluate ğ’‡(ğ±áµâºÂ¹), ğ’„â±¼(ğ±áµâºÂ¹).
		if spec.Line.Exact {
			ctx.line = int(findNoop)
			ss.exactSearch(math.NaN())
		} else {
			ctx.line = 0
			ctx.alpha = spec.Line.Alpha.Upper
			ss.inexactSearch()
			h3 *= ctx.alpha
		}

		for mode = evalFunc; mode == evalFunc; {
			mode = ss.lineSearch(&h3)
		}

		if mode == OK {
			return
		}

		// evaluate ğœµğ’‡(ğ±áµâºÂ¹), ğœµğ’„â±¼(ğ±áµâºÂ¹) and update BFGS
		if mode == evalGrad {
			mode = ss.updateBFGS()
		}
	}
	return
}

func (ss *sqpSolver) inexactSearch() {
	s, c, x := &ss.optimizer.sqpSpec, &ss.workspace.sqpCtx, ss.location.x
	c.line++
	dscal(s.n, c.alpha, c.s, 1) // ğ¬ = ğ±áµâºÂ¹ - ğ±áµ = ğ›‚ğ
	dcopy(s.n, c.x0, 1, x, 1)
	daxpy(s.n, one, c.s, 1, x, 1) // ğ±áµâºÂ¹ = ğ±áµ + ğ›‚ğ
	b, inf := s.Bounds, s.BndInf
	for i, v := range x {
		l, u := b[i].Lower, b[i].Upper
		if !math.IsNaN(l) && l > -inf && v < l {
			x[i] = l
		} else if !math.IsNaN(u) && u < inf && v > u {
			x[i] = u
		}
	}
}

func (ss *sqpSolver) exactSearch(t float64) (mode findMode) {
	s, c, x := &ss.optimizer.sqpSpec, &ss.workspace.sqpCtx, ss.location.x
	mode = findMode(c.line)
	if mode != findConv {
		c.alpha, mode = findMin(mode, &c.fw, t, c.tol, *s.Line.Alpha)
		c.line = int(mode)
		dcopy(s.n, c.x0, 1, x, 1)
		daxpy(s.n, c.alpha, c.s, 1, x, 1) // ğ± + ğ›‚ğ
	} else {
		dscal(s.n, c.alpha, c.s, 1) // ğ¬ = ğ±áµâºÂ¹ - ğ±áµ = ğ›‚ğ
	}
	return
}

func (ss *sqpSolver) lineSearch(h3 *float64) (mode sqpMode) {

	if mode = ss.evalLoc(evalFunc); mode != OK {
		return
	}

	spec, ctx, loc := &ss.optimizer.sqpSpec, &ss.workspace.sqpCtx, ss.location

	// Functions at the current x
	m, meq := spec.m, spec.meq

	// ğ¥(ğ±áµ+ğ›‚ğ;ğ›’) = ğ’‡(ğ±áµ+ğ›‚ğ) + ğ›’áµâ€–ğ’„(ğ±áµ+ğ›‚ğ)â€–â‚
	t := loc.f
	for j, c := range loc.c[:m] {
		h1 := zero
		if j < meq {
			h1 = c
		}
		t += ctx.mu[j] * math.Max(-c, h1)
	}

	li := spec.Line
	// Äˆğ‘£ğ‘–ğ‘œ = âˆ‘â€–ğ’„â±¼(ğ±áµ + ğ›‚ğ)â€–â‚
	// Äˆğ‘œğ‘ğ‘¡ = |ğ’‡(ğ±áµ + ğ›‚ğ) - ğ’‡(ğ±áµ)|
	if h1 := t - ctx.t0; !li.Exact {
		if h1 <= *h3/10 || ctx.line > 10 {
			*h3, mode = ss.checkConv(ctx.acc, evalGrad)
		} else {
			al, au := li.Alpha.Lower, li.Alpha.Upper
			ctx.alpha = math.Min(math.Max(*h3/(2*(*h3-h1)), al), au)
			// TODO: when alpha is NaN, it can be replaced with alpha.min (but seems not very helpful...)
			ss.inexactSearch()
			*h3 *= ctx.alpha
			mode = evalFunc
		}
	} else {
		if ss.exactSearch(t) == findConv {
			*h3, mode = ss.checkConv(ctx.acc, evalGrad)
		} else {
			mode = evalFunc
		}
	}
	return
}

// LSQ (Least Squares Quadratic programming) solves the problem
//
// minimize â€– ğƒÂ¹áŸÂ²ğ‹áµ€ğ± + ğƒâ»Â¹áŸÂ²ğ‹â»Â¹ğ  â€–â‚‚ subject to
//   - ğ€â±¼ğ± - ğ›â±¼ = 0  (j = 1 Â·Â·Â· mâ‚‘)
//   - ğ€â±¼ğ± - ğ›â±¼ â‰¥ 0  (j = mâ‚‘+1 Â·Â·Â· m)
//   - ğ’áµ¢ â‰¤ ğ±áµ¢ â‰¤ ğ’–áµ¢ (i = 1 Â·Â·Â· n)
//
// where
//   - ğ‹ is an n Ã— n lower triangular matrix with unit diagonal elements
//   - ğƒ is an n Ã— n diagonal matrix
//   - ğ  is an n-vector
//   - ğ€ is an m Ã— n matrix
//   - ğ› is an m-vector
//
// LSQ can be solved as LSEI problem ğš–ğš’ğš—â€– ğ„ğ± - ğŸ â€–â‚‚ subject to ğ‚ğ± = ğ and ğ†ğ± â‰¥ ğ¡ with:
//   - ğ„ = ğƒÂ¹áŸÂ²ğ‹áµ€
//   - ğŸ = -ğƒâ»Â¹áŸÂ²ğ‹â»Â¹ğ 
//   - ğ‚ = { ğ€â±¼: j = 1 Â·Â·Â· mâ‚‘ }
//   - ğ = { -ğ›â±¼: j = 1 Â·Â·Â· mâ‚‘ }
//   - ğ†â±¼ = { ğ€â±¼: j = mâ‚‘+1 Â·Â·Â· m }
//   - ğ¡â±¼ = { -ğ›â±¼: j = mâ‚‘+1 Â·Â·Â· m }
//
// and the bounds is equivalent to inequality constraints ğˆğ± â‰¥ ğ’ and -ğˆğ± â‰¥ -ğ’– such that:
//   - ğ†â±¼ = { ğˆâ±¼: j = m+1 Â·Â·Â· m+n }
//   - ğ¡â±¼ = { ğ’â±¼: j = m+1 Â·Â·Â· m+n }
//   - ğ†â±¼ = { -ğˆâ±¼: j = m+n Â·Â·Â· m+2n }
//   - ğ¡â±¼ = { -ğ’–â±¼: j = m+n Â·Â·Â· m+2n }
//
// where
//   - ğ„ is an n Ã— n upper triangular matrix
//   - ğŸ is an n-vector
//   - ğ‚ is an mâ‚‘ Ã— n matrix
//   - ğ is an mâ‚‘-vector
//   - ğ† is an (m-mâ‚‘+2n) Ã— n matrix
//   - ğ¡ is an (m-mâ‚‘+2n)-vector
func LSQ(m, meq, n, nl int,
	// l(nl) = ğ‹ + ğƒ
	// g(n) = ğ 
	// a(m,n) = ğ€
	// b(m) = ğ›
	// xl(n), xu(n) = ğ’, ğ’–
	l, g, a, b, xl, xu []float64,
	// x(n) : solution vector
	// y(m+n+n) : lagrange multiplier (constraints, lower+upper bounds)
	x, y []float64,
	// w, jw : temporary workspace
	w []float64, jw []int,
	maxIter int, infBnd float64) (float64, sqpMode) {

	mineq := m - meq
	m1 := mineq + n + n // ine
	la := max(m, 1)

	// Determine problem type
	var n1, n2, n3 int
	n1 = n + 1
	if (n+1)*n/2+1 == nl {
		// Solve the origin problem m Ã— n
		n2, n3 = 0, n
	} else {
		// Solve the augmented problem m Ã— (n+1)
		n2, n3 = 1, n-1
	}

	e0, f0 := 0, n*n                // Start index of E and f
	c0, d0 := f0+n, (f0+n)+meq*n    // Start index of C and d
	g0, h0 := d0+meq, (d0+meq)+m1*n // Start index of G and h
	w0 := h0 + m1                   // Start index of workspace

	// Recover matrix E and vector F from l and g
	i2, i3, i4 := 0, 0, 0
	for j := 0; j < n3; j++ {
		i := n - j
		diag := math.Sqrt(l[i2]) // ğƒÂ¹áŸÂ²
		dzero(w[i3 : i3+i])
		dcopy(i-n2, l[i2:], 1, w[i3:], n) // ğ„â±¼ = ğ‹â±¼áµ€
		dscal(i-n2, diag, w[i3:], n)      //  ğ„â±¼ = ğƒÂ¹áŸÂ²ğ‹â±¼áµ€
		w[i3] = diag                      //  ğ„â±¼â±¼ = ğƒÂ¹áŸÂ²â±¼â±¼
		// ğ² = ğ‹â»Â¹ğ   â†’  ğ²â±¼ = (ğ â±¼ - âˆ‘áµ¢ğ‹â±¼áµ¢ğ²áµ¢) / ğ‹â±¼â±¼
		// ğ‹â±¼â±¼ = 1   â†’  (ğ‹â»Â¹ğ )â±¼ = (ğ â±¼ - âˆ‘áµ¢ğ‹â±¼áµ¢ğ²áµ¢)
		w[f0+j] = (g[j] - ddot(j, w[i4:], 1, w[f0:], 1)) / diag // ğŸâ±¼ = ğƒâ»Â¹áŸÂ²â±¼â±¼(ğ‹â»Â¹ğ )â±¼
		i2 += i - n2
		i3 += n1
		i4 += n
	}
	if n2 == 1 {
		w[i3] = l[nl-1]      // ğ„â±¼â±¼ = ğ›’
		dzero(w[i4 : i4+n3]) //
		w[f0+n3] = zero      // ğŸâ±¼ = 0
	}
	dscal(n, -one, w[f0:f0+n], 1) // ğŸâ±¼ = -ğƒâ»Â¹áŸÂ²ğ‹â»Â¹ğ 

	if meq > 0 {
		// Recover matrix C from upper part of A
		for i := 0; i < meq; i++ {
			dcopy(n, a[i:], la, w[c0+i:], meq) // ğ‚â±¼ = ğ€â±¼ = - ğ’„â±¼(ğ±áµ)
		}
		// Recover vector d from upper part of b
		dcopy(meq, b, 1, w[d0:], 1) // ğâ±¼ = -ğ›â±¼ = -ğ’„â±¼(ğ±áµ)
		dscal(meq, -one, w[d0:], 1)
	}

	if mineq > 0 {
		// Recover matrix G from lower part of A
		for i := 0; i < mineq; i++ {
			dcopy(n, a[meq+i:], la, w[g0+i:], m1) // ğ†â±¼ = ğ€â±¼ = - ğ’„â±¼(ğ±áµ)
		}
		// Recover vector h from lower part of b
		dcopy(mineq, b[meq:], 1, w[h0:], 1) // ğ¡â±¼ = -ğ›â±¼ = -ğ’„â±¼(ğ±áµ)
		dscal(mineq, -one, w[h0:], 1)
	}

	// Augment matrix G with Â±ğˆ
	// Recover vector h from bounds
	bnd := mineq
	xl, xu = xl[:n], xu[:n]
	for i, l := range xl {
		if !math.IsNaN(l) && l > -infBnd {
			ip, il := g0+bnd, h0+bnd
			w[il] = l    // ğ¡â±¼ = ğ’â±¼
			w[ip] = zero // ğ†â±¼ = ğˆâ±¼
			dcopy(n, w[ip:], 0, w[ip:], m1)
			w[ip+m1*i] = one
			bnd++
		}
	}
	for i, u := range xu {
		if !math.IsNaN(u) && u < infBnd {
			ip, il := g0+bnd, h0+bnd
			w[il] = -u   // ğ¡â±¼ = -ğ’–â±¼
			w[ip] = zero // ğ†â±¼ = -ğˆâ±¼
			dcopy(n, w[ip:], 0, w[ip:], m1)
			w[ip+m1*i] = -one
			bnd++
		}
	}

	nan := (n + n) - (bnd - mineq)
	norm, mode := LSEI(w[c0:d0], w[d0:g0], w[e0:f0], w[f0:c0], w[g0:h0], w[h0:w0], max(1, meq), meq, n, n, m1, m1-nan, n, x, w[w0:], jw, maxIter)

	if mode == HasSolution {
		// Restore Lagrange multipliers
		dcopy(m, w[w0:], 1, y, 1)
		if n3 > 0 {
			// Set unused multipliers to NaN
			y[m] = math.NaN()
			dcopy(n3+n3, y[m:], 0, y[m:], 1)
		}
		for i, l := range xl {
			if !math.IsNaN(l) && l > -infBnd && x[i] < l {
				x[i] = l
			}
		}
		for i, u := range xu {
			if !math.IsNaN(u) && u < infBnd && x[i] > u {
				x[i] = u
			}
		}
	}
	return norm, mode
}
